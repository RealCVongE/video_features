{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈로 불러다가 Feature값 추출하기 - 4초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 필요한거 추려서 주피터 셀별로 실행하기 - 2초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫 번째 셀: 클래스와 필요한 라이브러리 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리와 모듈을 임포트합니다.\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from typing import Dict\n",
    "from models.i3d.i3d_src.i3d_net import I3D  # I3D 모델 구현\n",
    "from models._base.base_extractor import BaseExtractor  # 기본 특징 추출기 클래스를 상속\n",
    "from models.raft.raft_src.raft import RAFT, InputPadder\n",
    "from models.transforms import (Clamp, PILToTensor, ResizeImproved, ScaleTo1_1, TensorCenterCrop, ToFloat, PermuteAndUnsqueeze, ToUInt8)\n",
    "from utils.io import reencode_video_with_diff_fps  # 동영상 FPS 조정 유틸리티\n",
    "from utils.utils import dp_state_to_normal, show_predictions_on_dataset\n",
    "from models.raft.extract_raft import DATASET_to_RAFT_CKPT_PATHS\n",
    "\n",
    "# I3D 모델을 사용해 RGB 데이터만 추출하는 클래스\n",
    "class ExtractI3D(BaseExtractor):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__(\n",
    "            feature_type=args['feature_type'],\n",
    "            on_extraction=args.get('on_extraction', 'save_numpy'),\n",
    "            tmp_path=args['tmp_path'],\n",
    "            output_path=args['output_path'],\n",
    "            keep_tmp_files=args['keep_tmp_files'],\n",
    "            device=args['device'],\n",
    "        )\n",
    "        self.flow_type = args.flow_type\n",
    "        self.streams = ['rgb',\"flow\"]\n",
    "        self.i3d_classes_num = 400\n",
    "        self.min_side_size = 256\n",
    "        self.central_crop_size = 224\n",
    "        self.extraction_fps = args.get('extraction_fps', None)\n",
    "        self.step_size = args.get('step_size', 16)\n",
    "        self.stack_size = args.get('stack_size', 16)\n",
    "        self.resize_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            ResizeImproved(self.min_side_size),\n",
    "            PILToTensor(),\n",
    "            ToFloat(),\n",
    "        ])\n",
    "        self.i3d_transforms = {\n",
    "            'rgb': torchvision.transforms.Compose([\n",
    "                TensorCenterCrop(self.central_crop_size),\n",
    "                ScaleTo1_1(),\n",
    "                PermuteAndUnsqueeze()\n",
    "            ]),\n",
    "            'flow': torchvision.transforms.Compose([\n",
    "                TensorCenterCrop(self.central_crop_size),\n",
    "                Clamp(-20, 20),\n",
    "                ToUInt8(),\n",
    "                ScaleTo1_1(),\n",
    "                PermuteAndUnsqueeze()\n",
    "            ])\n",
    "        }\n",
    "        self.name2module = self.load_model()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract(self, video_path: str) -> Dict[str, np.ndarray]:\n",
    "        if self.extraction_fps is not None:\n",
    "            video_path = reencode_video_with_diff_fps(video_path, self.tmp_path, self.extraction_fps)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        rgb_stack = []\n",
    "        feats_dict = {'rgb': [],\"flow\":[]}\n",
    "        padder = None\n",
    "        first_frame = True\n",
    "        while cap.isOpened():\n",
    "            frame_exists, rgb = cap.read()\n",
    "\n",
    "            if first_frame:\n",
    "                first_frame = False\n",
    "                if not frame_exists:\n",
    "                    continue\n",
    "\n",
    "            if frame_exists:\n",
    "                rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "                rgb = self.resize_transforms(rgb)\n",
    "                rgb = rgb.unsqueeze(0)\n",
    "                if self.flow_type == 'raft' and padder is None:\n",
    "                    padder = InputPadder(rgb.shape)\n",
    "\n",
    "                rgb_stack.append(rgb)\n",
    "\n",
    "                if len(rgb_stack) == self.stack_size + 1:\n",
    "                    batch_feats_dict = self.run_on_a_stack(rgb_stack,padder)\n",
    "                    for stream in self.streams:\n",
    "                        feats_dict[stream].extend(batch_feats_dict[stream].tolist())\n",
    "                    rgb_stack = rgb_stack[self.step_size:]  # 스텝 사이즈만큼 스택에서 제거\n",
    "            else:\n",
    "                cap.release()\n",
    "                break\n",
    "\n",
    "        if (self.extraction_fps is not None) and (not self.keep_tmp_files):\n",
    "            os.remove(video_path)\n",
    "\n",
    "        feats_dict = {stream: np.array(feats) for stream, feats in feats_dict.items()}\n",
    "        return feats_dict\n",
    "\n",
    "    def run_on_a_stack(self, rgb_stack, padder=None) -> Dict[str, torch.Tensor]:\n",
    "        models = self.name2module['model']\n",
    "        flow_xtr_model = self.name2module.get('flow_xtr_model', None)\n",
    "        rgb_stack = torch.cat(rgb_stack).to(self.device)\n",
    "\n",
    "        batch_feats_dict = {}\n",
    "        for stream in self.streams:\n",
    "            # if i3d stream is flow, we first need to calculate optical flow, otherwise, we use rgb\n",
    "            # `end_idx-1` and `start_idx+1` because flow is calculated between f and f+1 frames\n",
    "            # we also use `end_idx-1` for stream == 'rgb' case: just to make sure the feature length\n",
    "            # is same regardless of whether only rgb is used or flow\n",
    "            if stream == 'flow':\n",
    "                if self.flow_type == 'raft':\n",
    "                    stream_slice = flow_xtr_model(padder.pad(rgb_stack)[:-1], padder.pad(rgb_stack)[1:])\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            elif stream == 'rgb':\n",
    "                stream_slice = rgb_stack[:-1]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            # apply transforms depending on the stream (flow or rgb)\n",
    "            stream_slice = self.i3d_transforms[stream](stream_slice)\n",
    "            # extract features for a stream\n",
    "            batch_feats_dict[stream] = models[stream](stream_slice, features=True)  # (B, 1024)\n",
    "            # add features to the output dict\n",
    "\n",
    "        return batch_feats_dict\n",
    "\n",
    "    def load_model(self) -> Dict[str, torch.nn.Module]:\n",
    "        \"\"\"Defines the models, loads checkpoints, sends them to the device.\n",
    "        Since I3D is two-stream, it may load a optical flow extraction model as well.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.nn.Module]: model-agnostic dict holding modules for extraction and show_pred\n",
    "        \"\"\"\n",
    "        flow_model_paths = {'raft': DATASET_to_RAFT_CKPT_PATHS['sintel'], }\n",
    "        i3d_weights_paths = {\n",
    "            'rgb': './models/i3d/checkpoints/i3d_rgb.pt',\n",
    "            'flow': './models/i3d/checkpoints/i3d_flow.pt',\n",
    "        }\n",
    "        name2module = {}\n",
    "\n",
    "        if \"flow\" in self.streams:\n",
    "            # Flow extraction module\n",
    "            if self.flow_type == 'raft':\n",
    "                flow_xtr_model = RAFT()\n",
    "            else:\n",
    "                raise NotImplementedError(f'Flow model {self.flow_type} is not implemented')\n",
    "            # Preprocess state dict\n",
    "            state_dict = torch.load(flow_model_paths[self.flow_type], map_location='cpu')\n",
    "            state_dict = dp_state_to_normal(state_dict)\n",
    "            flow_xtr_model.load_state_dict(state_dict)\n",
    "            flow_xtr_model = flow_xtr_model.to(self.device)\n",
    "            flow_xtr_model.eval()\n",
    "            name2module['flow_xtr_model'] = flow_xtr_model\n",
    "\n",
    "        # Feature extraction models (rgb and flow streams)\n",
    "        i3d_stream_models = {}\n",
    "        for stream in self.streams:\n",
    "            i3d_stream_model = I3D(num_classes=self.i3d_classes_num, modality=stream)\n",
    "            i3d_stream_model.load_state_dict(torch.load(i3d_weights_paths[stream], map_location='cpu'))\n",
    "            i3d_stream_model = i3d_stream_model.to(self.device)\n",
    "            i3d_stream_model.eval()\n",
    "            i3d_stream_models[stream] = i3d_stream_model\n",
    "        name2module['model'] = i3d_stream_models\n",
    "\n",
    "        return name2module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 번째 셀: 설정과 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 설정을 담은 args 딕셔너리를 정의합니다.\n",
    "# 이 설정은 ExtractI3D 클래스를 사용하기 위해 필요한 정보를 포함합니다.\n",
    "# 실제 경로 및 설정은 사용 환경에 맞게 조정해야 합니다.\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "args = OmegaConf.create({\n",
    "    'feature_type': 'i3d',\n",
    "    'device': 'cuda:0',  # 또는 'cpu'를 사용하시면 됩니다.\n",
    "    'on_extraction': 'ignore',  # 이 설정은 무시될 것입니다.\n",
    "    'output_path': 'ignore',  # 이 설정도 무시됩니다.\n",
    "    'stack_size': 16,  # 이 값들은 예시이며 실제 값으로 대체해야 합니다.\n",
    "    'step_size': 16,\n",
    "    'streams': None,\n",
    "    'flow_type': 'raft',\n",
    "    'extraction_fps': 25,\n",
    "    'tmp_path': './tmp/i3d',\n",
    "    'keep_tmp_files': False,\n",
    "    'show_pred': False,\n",
    "    'config': None\n",
    "    # 여기에 args_cli에 필요한 나머지 설정을 추가하십시오.\n",
    "})\n",
    "\n",
    "# ExtractI3D 클래스의 인스턴스를 생성합니다.\n",
    "# 위에서 정의한 args 딕셔너리를 생성자에 전달합니다.\n",
    "extractor = ExtractI3D(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세 번째 셀: 동영상 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 RGB 특징의 크기: (52, 1024)\n",
      "추출된 flow 특징의 크기: (52, 1024)\n",
      "추출된 flow 특징의 크기: (52, 2048)\n",
      "10.7276771068573\n",
      "[[0.32722583 1.25116682 0.31964004 ... 0.1165202  0.17485085 0.26154089]\n",
      " [0.69086689 0.75427979 0.40393949 ... 0.4031609  0.01892482 0.57530981]\n",
      " [0.55582756 0.77099133 0.44683546 ... 0.11136977 0.21072075 0.40609398]\n",
      " ...\n",
      " [0.25432694 0.46524847 0.12953079 ... 0.20917004 0.00383476 0.36249563]\n",
      " [0.42517626 0.41043964 0.16410686 ... 0.11223916 0.17202944 0.56375074]\n",
      " [0.11324847 0.55317342 0.16152973 ... 0.43312672 0.25188816 0.13269226]]\n"
     ]
    }
   ],
   "source": [
    "# 동영상 파일 경로를 지정합니다.\n",
    "# 이 경로는 실제로 특징을 추출하고자 하는 동영상 파일의 위치를 가리킵니다.\n",
    "# 사용자 환경에 맞게 해당 경로를 수정해야 합니다.\n",
    "import time\n",
    "\n",
    "\n",
    "video_path = '/home/ubuntu/video_features/video_test_0000004.mp4'\n",
    "\n",
    "# 앞서 생성한 ExtractI3D 인스턴스의 extract 메소드를 호출하여\n",
    "# 지정된 동영상 파일에서 RGB 특징을 추출합니다.\n",
    "# 이 메소드는 추출된 특징들을 사전 형태로 반환합니다.\n",
    "start_time= time.time()\n",
    "features = extractor.extract(video_path)\n",
    "\n",
    "# 추출된 RGB 특징을 numpy 배열로 받아서 변수에 저장합니다.\n",
    "# 'rgb' 키를 사용하여 사전에서 RGB 특징에 접근할 수 있습니다.\n",
    "rgb_features = features['rgb']\n",
    "flow_features = features['flow']\n",
    "concatenated_features = np.concatenate((rgb_features, flow_features), axis=1)\n",
    "end_time= time.time()\n",
    "\n",
    "# 선택적: 추출된 특징의 크기나 내용을 확인하기 위해 print 함수를 사용할 수 있습니다.\n",
    "print(f\"추출된 RGB 특징의 크기: {rgb_features.shape}\")\n",
    "print(f\"추출된 flow 특징의 크기: {flow_features.shape}\")\n",
    "print(f\"추출된 flow 특징의 크기: {concatenated_features.shape}\")\n",
    "np.save('video4.npy', concatenated_features) \n",
    "print(end_time-start_time)\n",
    "print(concatenated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_features",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
